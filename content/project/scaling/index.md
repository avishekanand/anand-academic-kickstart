---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: "Large Scale Machine Learning"
summary: "My research aims to develop intelligent and transparent machine learning approaches to help humans find relevant information. "
authors: []
tags: []
categories: []
date: 2020-07-19T11:18:06+02:00

# Optional external URL for project (replaces project detail page).
external_link: ""

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: ""
  preview_only: false

# Custom links (optional).
#   Uncomment and edit lines below to show custom links.
# links:
# - name: Follow
#   url: https://twitter.com
#   icon_pack: fab
#   icon: twitter

url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides: ""
---

Machine learning models are progressively becoming complex and training datasets are getting larger by the data. Embeddings models are trained over Web scale collections of text and graphs, language models are learnt over millions or billions of sentences. It is fair to say that training machine learning models over huge datasets has reaped massive benefits over the last few years in downstream tasks but on the other hand has become computationally expensive. One of our goals is how to build large models -- embeddings models and language models over massive collections of text and graphs -- in reasonable time and modest academic hardware resources. In this stream of research we focus on investigating learning regimes using distributed and approximate algorithms to greatly improve training times of such large machine learning models.


## Publications

[1] **[Asynchronous Training of Word Embeddings for Large Text Corpora](https://doi.org/10.1145/3289600.3291011)**. A. Anand, M. Khosla, J. Singh, J. Zab, Z. Zhang. <em>In International Conference on Web Search and Data Mining, WSDM 2019</em>.

[2] **[Node Representation Learning for Directed Graphs](https://doi.org/10.1007/978-3-030-46150-8_24)**.  M. Khosla, J. Leonhardt, W. Nejdl, A. Anand. <em>Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2019</em>.
